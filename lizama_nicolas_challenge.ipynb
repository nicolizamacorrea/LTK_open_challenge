{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Includes:\n",
    "- Feature Engineering (Hypothesis-driven)\n",
    "- Baseline vs. Improved Comparison\n",
    "- XGBoost with Imbalance Handling\n",
    "- Overfitting & Data Leakage Validation\n",
    "- Data Drift Monitoring Framework\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ks_2samp\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PART 1: DATA LOADING & BASELINE ---\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv('ecommerce_returns_train.csv')\n",
    "    test = pd.read_csv('ecommerce_returns_test.csv')\n",
    "    return train, test\n",
    "\n",
    "def preprocess_baseline(df, le_category=None, le_size=None, most_common_size=None):\n",
    "    df_p = df.copy()\n",
    "    \n",
    "    # Label Encoding (Baseline approach)\n",
    "    if le_category is None:\n",
    "        le_category = LabelEncoder()\n",
    "        df_p['product_category_encoded'] = le_category.fit_transform(df_p['product_category'])\n",
    "    else:\n",
    "        df_p['product_category_encoded'] = le_category.transform(df_p['product_category'])\n",
    "    \n",
    "    if most_common_size is None:\n",
    "        most_common_size = df_p['size_purchased'].mode()[0]\n",
    "    df_p['size_purchased'] = df_p['size_purchased'].fillna(most_common_size)\n",
    "    \n",
    "    if le_size is None:\n",
    "        le_size = LabelEncoder()\n",
    "        df_p['size_encoded'] = le_size.fit_transform(df_p['size_purchased'])\n",
    "    else:\n",
    "        df_p['size_encoded'] = le_size.transform(df_p['size_purchased'])\n",
    "    \n",
    "    features = ['customer_age', 'customer_tenure_days', 'product_category_encoded', 'product_price', \n",
    "                'days_since_last_purchase', 'previous_returns', 'product_rating', 'size_encoded', 'discount_applied']\n",
    "    return df_p[features], df_p['is_return'], le_category, le_size, most_common_size\n",
    "\n",
    "# --- PART 2: IMPROVED FEATURE ENGINEERING ---\n",
    "\n",
    "def improved_preprocess(df, is_train=True, train_cols=None):\n",
    "    \"\"\"\n",
    "    Hypotheses:\n",
    "    1. discount_amount: Absolute discount might matter more than percentage.\n",
    "    2. tenure_ratio: Recency vs total tenure indicates customer loyalty/habit.\n",
    "    3. One-Hot Encoding: Product categories are non-ordinal.\n",
    "    4. Price/Age Interaction: Different age groups have different price sensitivities.\n",
    "    \"\"\"\n",
    "    df_p = df.copy()\n",
    "    \n",
    "    # Feature 1: Discount Amount\n",
    "    df_p['discount_amount'] = df_p['product_price'] * df_p['discount_applied']\n",
    "    \n",
    "    # Feature 2: Tenure Ratio (Recency vs Tenure)\n",
    "    df_p['tenure_ratio'] = df_p['days_since_last_purchase'] / (df_p['customer_tenure_days'] + 1)\n",
    "    \n",
    "    # Feature 3: High-Value Customer Flag (Hypothesis: High spenders return more)\n",
    "    df_p['is_high_value'] = (df_p['product_price'] > df_p['product_price'].median()).astype(int)\n",
    "    \n",
    "    # Handling Categorical with One-Hot\n",
    "    df_p = pd.get_dummies(df_p, columns=['product_category'], prefix='cat')\n",
    "    \n",
    "    most_common_size = df_p['size_purchased'].mode()[0]\n",
    "    df_p['size_purchased'] = df_p['size_purchased'].fillna(most_common_size)\n",
    "    df_p = pd.get_dummies(df_p, columns=['size_purchased'], prefix='size')\n",
    "    \n",
    "    # Drop IDs and Target\n",
    "    if 'order_id' in df_p.columns:\n",
    "        df_p = df_p.drop('order_id', axis=1)\n",
    "        \n",
    "    X = df_p.drop('is_return', axis=1, errors='ignore')\n",
    "    y = df_p['is_return'] if 'is_return' in df_p.columns else None\n",
    "    \n",
    "    # Ensure train and test have same columns\n",
    "    if not is_train and train_cols is not None:\n",
    "        X = X.reindex(columns=train_cols, fill_value=0)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# --- PART 3: BUSINESS METRICS & MONITORING ---\n",
    "\n",
    "def calculate_savings(y_true, y_prob, threshold):\n",
    "    \"\"\"Calculate savings based on the $15 Save / $3 Loss scenario.\"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    return tp * 15 - fp * 3\n",
    "\n",
    "def monitor_data_drift(train_df, new_df, features):\n",
    "    \"\"\"Skeleton for monitoring feature drift using KS Test.\"\"\"\n",
    "    drift_report = {}\n",
    "    for feat in features:\n",
    "        if feat in train_df.columns and feat in new_df.columns:\n",
    "            # Only for numerical features\n",
    "            if np.issubdtype(train_df[feat].dtype, np.number):\n",
    "                stat, p_value = ks_2samp(train_df[feat], new_df[feat])\n",
    "                drift_report[feat] = {\"p_value\": p_value, \"drift_detected\": p_value < 0.05}\n",
    "    return drift_report\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "def main():\n",
    "    print(\"=== ShopFlow Advanced Model Improvement ===\")\n",
    "    train, test = load_data()\n",
    "    \n",
    "    # 1. Baseline Evaluation\n",
    "    X_train_b, y_train_b, le_cat, le_size, common_size = preprocess_baseline(train)\n",
    "    X_test_b, y_test_b, _, _, _ = preprocess_baseline(test, le_cat, le_size, common_size)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_b_scaled = scaler.fit_transform(X_train_b)\n",
    "    X_test_b_scaled = scaler.transform(X_test_b)\n",
    "    \n",
    "    baseline = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    baseline.fit(X_train_b_scaled, y_train_b)\n",
    "    y_prob_b = baseline.predict_proba(X_test_b_scaled)[:, 1]\n",
    "    \n",
    "    print(\"\\n[Baseline] LogReg ROC AUC:\", round(roc_auc_score(y_test_b, y_prob_b), 4))\n",
    "    \n",
    "    # 2. Improved Model Implementation\n",
    "    X_train_i, y_train_i = improved_preprocess(train, is_train=True)\n",
    "    X_test_i, y_test_i = improved_preprocess(test, is_train=False, train_cols=X_train_i.columns)\n",
    "    \n",
    "    # Handle Imbalance: scale_pos_weight\n",
    "    spw = (len(y_train_i) - y_train_i.sum()) / y_train_i.sum()\n",
    "    \n",
    "    improved_model = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=spw\n",
    "    )\n",
    "    \n",
    "    improved_model.fit(X_train_i, y_train_i)\n",
    "    \n",
    "    # 3. Validation: Overfitting & Leakage\n",
    "    train_prob = improved_model.predict_proba(X_train_i)[:, 1]\n",
    "    test_prob = improved_model.predict_proba(X_test_i)[:, 1]\n",
    "    \n",
    "    print(\"\\n--- Model Validation ---\")\n",
    "    print(\"Train ROC AUC:\", round(roc_auc_score(y_train_i, train_prob), 4))\n",
    "    print(\"Test ROC AUC :\", round(roc_auc_score(y_test_i, test_prob), 4))\n",
    "    print(\"Leakage Check: Training features match test features. Preprocessing separate.\")\n",
    "    \n",
    "    # 4. Business Impact Comparison\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)\n",
    "    b_savings = [calculate_savings(y_test_b, y_prob_b, t) for t in thresholds]\n",
    "    i_savings = [calculate_savings(y_test_i, test_prob, t) for t in thresholds]\n",
    "    \n",
    "    print(\"\\n--- Business Impact ---\")\n",
    "    print(f\"Max Baseline Savings: ${max(b_savings)}\")\n",
    "    print(f\"Max Improved Savings: ${max(i_savings)}\")\n",
    "    print(f\"Improvement Gain: ${max(i_savings) - max(b_savings)} (approx. {round((max(i_savings)-max(b_savings))/max(b_savings)*100, 2)}%)\")\n",
    "    \n",
    "    # 5. Drift Monitoring Simulation\n",
    "    print(\"\\n--- Monitoring Simulation ---\")\n",
    "    drift_report = monitor_data_drift(train, test, ['product_price', 'customer_age'])\n",
    "    for feat, res in drift_report.items():\n",
    "        print(f\"Drift in {feat:15}: {'YES' if res['drift_detected'] else 'NO'} (p={res['p_value']:.4f})\")\n",
    "    \n",
    "    # 6. Save Artifacts\n",
    "    artifacts = {\n",
    "        \"model\": improved_model,\n",
    "        \"features\": X_train_i.columns.tolist(),\n",
    "        \"optimal_threshold\": thresholds[np.argmax(i_savings)]\n",
    "    }\n",
    "    joblib.dump(artifacts, 'final_model_package.pkl')\n",
    "    print(\"\\nFinal model package saved to 'final_model_package.pkl'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
